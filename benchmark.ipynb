{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2024-04-24T02:00:53.631124Z",
     "iopub.status.busy": "2024-04-24T02:00:53.630786Z",
     "iopub.status.idle": "2024-04-24T02:00:55.689613Z",
     "shell.execute_reply": "2024-04-24T02:00:55.688661Z",
     "shell.execute_reply.started": "2024-04-24T02:00:53.631056Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:19:52.115546300Z",
     "start_time": "2024-05-10T01:19:51.048653Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pkgutil' has no attribute 'ImpImporter'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpkg_resources\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtime\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pkg_resources\\__init__.py:2158\u001B[0m\n\u001B[0;32m   2154\u001B[0m     dist_groups \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(find_distributions, resolved_paths)\n\u001B[0;32m   2155\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(dist_groups, ())\n\u001B[1;32m-> 2158\u001B[0m register_finder(\u001B[43mpkgutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mImpImporter\u001B[49m, find_on_path)\n\u001B[0;32m   2160\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(importlib_machinery, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFileFinder\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m   2161\u001B[0m     register_finder(importlib_machinery\u001B[38;5;241m.\u001B[39mFileFinder, find_on_path)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'pkgutil' has no attribute 'ImpImporter'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pkg_resources\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and pre-process the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:00:55.693553Z",
     "iopub.status.busy": "2024-04-24T02:00:55.693292Z",
     "iopub.status.idle": "2024-04-24T02:01:23.715538Z",
     "shell.execute_reply": "2024-04-24T02:01:23.714810Z",
     "shell.execute_reply.started": "2024-04-24T02:00:55.693499Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2024-05-10T01:19:52.108463500Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "print('loaded %d records' % len(train))\n",
    "\n",
    "# Make sure all comment_text values are strings\n",
    "train['comment_text'] = train['comment_text'].astype(str) \n",
    "\n",
    "# List all identities\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "# Convert taget and identity columns to booleans\n",
    "def convert_to_bool(df, col_name):\n",
    "    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n",
    "    \n",
    "def convert_dataframe_to_bool(df):\n",
    "    bool_df = df.copy()\n",
    "    for col in ['target'] + identity_columns:\n",
    "        convert_to_bool(bool_df, col)\n",
    "    return bool_df\n",
    "\n",
    "train = convert_dataframe_to_bool(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into 80% train and 20% validate sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:01:23.717481Z",
     "iopub.status.busy": "2024-04-24T02:01:23.717163Z",
     "iopub.status.idle": "2024-04-24T02:01:25.493770Z",
     "shell.execute_reply": "2024-04-24T02:01:25.492828Z",
     "shell.execute_reply.started": "2024-04-24T02:01:23.717421Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2024-05-10T01:19:52.111307300Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df, validate_df = model_selection.train_test_split(train, test_size=0.2)\n",
    "print('%d train comments, %d validate comments' % (len(train_df), len(validate_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a text tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:01:25.495531Z",
     "iopub.status.busy": "2024-04-24T02:01:25.495199Z",
     "iopub.status.idle": "2024-04-24T02:03:01.848534Z",
     "shell.execute_reply": "2024-04-24T02:03:01.847824Z",
     "shell.execute_reply.started": "2024-04-24T02:01:25.495454Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2024-05-10T01:19:52.112232300Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 10000\n",
    "TOXICITY_COLUMN = 'target'\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "\n",
    "# Create a text tokenizer.\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_df[TEXT_COLUMN])\n",
    "\n",
    "# All comments must be truncated or padded to be the same length.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "def pad_text(texts, tokenizer):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train a Convolutional Neural Net for classifying toxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:03:01.850248Z",
     "iopub.status.busy": "2024-04-24T02:03:01.850018Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2024-05-10T01:19:52.113303200Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDINGS_PATH = '../input/glove.6B.100d.txt'\n",
    "EMBEDDINGS_DIMENSION = 100\n",
    "DROPOUT_RATE = 0.3\n",
    "LEARNING_RATE = 0.00005\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def train_model(train_df, validate_df, tokenizer):\n",
    "    # Prepare data\n",
    "    train_text = pad_text(train_df[TEXT_COLUMN], tokenizer)\n",
    "    train_labels = to_categorical(train_df[TOXICITY_COLUMN])\n",
    "    validate_text = pad_text(validate_df[TEXT_COLUMN], tokenizer)\n",
    "    validate_labels = to_categorical(validate_df[TOXICITY_COLUMN])\n",
    "\n",
    "    # Load embeddings\n",
    "    print('loading embeddings')\n",
    "    embeddings_index = {}\n",
    "    with open(EMBEDDINGS_PATH) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1,\n",
    "                                 EMBEDDINGS_DIMENSION))\n",
    "    num_words_in_embedding = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            num_words_in_embedding += 1\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # Create model layers.\n",
    "    def get_convolutional_neural_net_layers():\n",
    "        \"\"\"Returns (input_layer, output_layer)\"\"\"\n",
    "        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                                    EMBEDDINGS_DIMENSION,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "        x = embedding_layer(sequence_input)\n",
    "        x = Conv1D(128, 2, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(5, padding='same')(x)\n",
    "        x = Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(5, padding='same')(x)\n",
    "        x = Conv1D(128, 4, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(40, padding='same')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dropout(DROPOUT_RATE)(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        preds = Dense(2, activation='softmax')(x)\n",
    "        return sequence_input, preds\n",
    "\n",
    "    # Compile model.\n",
    "    print('compiling model')\n",
    "    input_layer, output_layer = get_convolutional_neural_net_layers()\n",
    "    model = Model(input_layer, output_layer)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(lr=LEARNING_RATE),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    # Train model.\n",
    "    print('training model')\n",
    "    model.fit(train_text,\n",
    "              train_labels,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=NUM_EPOCHS,\n",
    "              validation_data=(validate_text, validate_labels),\n",
    "              verbose=2)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_model(train_df, validate_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate model predictions on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:24.763588400Z",
     "start_time": "2024-05-10T01:21:24.717608200Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m MODEL_NAME \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmy_model\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 2\u001B[0m validate_df[MODEL_NAME] \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mpredict(pad_text(validate_df[TEXT_COLUMN], tokenizer))[:, \u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'my_model'\n",
    "validate_df[MODEL_NAME] = model.predict(pad_text(validate_df[TEXT_COLUMN], tokenizer))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:25.355636800Z",
     "start_time": "2024-05-10T01:21:25.292009400Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validate_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mvalidate_df\u001B[49m\u001B[38;5;241m.\u001B[39mhead()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'validate_df' is not defined"
     ]
    }
   ],
   "source": [
    "validate_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define bias metrics, then evaluate our new model for bias using the validation set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:26.857787800Z",
     "start_time": "2024-05-10T01:21:26.762961500Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validate_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 47\u001B[0m\n\u001B[0;32m     44\u001B[0m         records\u001B[38;5;241m.\u001B[39mappend(record)\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mDataFrame(records)\u001B[38;5;241m.\u001B[39msort_values(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubgroup_auc\u001B[39m\u001B[38;5;124m'\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 47\u001B[0m bias_metrics_df \u001B[38;5;241m=\u001B[39m compute_bias_metrics_for_model(\u001B[43mvalidate_df\u001B[49m, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n\u001B[0;32m     48\u001B[0m bias_metrics_df\n",
      "\u001B[1;31mNameError\u001B[0m: name 'validate_df' is not defined"
     ]
    }
   ],
   "source": [
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, model_name):\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n",
    "    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df[df[subgroup] & df[label]]\n",
    "    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset[dataset[subgroup]])\n",
    "        }\n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n",
    "\n",
    "bias_metrics_df = compute_bias_metrics_for_model(validate_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "bias_metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the final score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:27.665451100Z",
     "start_time": "2024-05-10T01:21:27.631398500Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bias_metrics_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 18\u001B[0m\n\u001B[0;32m     11\u001B[0m     bias_score \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39maverage([\n\u001B[0;32m     12\u001B[0m         power_mean(bias_df[SUBGROUP_AUC], POWER),\n\u001B[0;32m     13\u001B[0m         power_mean(bias_df[BPSN_AUC], POWER),\n\u001B[0;32m     14\u001B[0m         power_mean(bias_df[BNSP_AUC], POWER)\n\u001B[0;32m     15\u001B[0m     ])\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (OVERALL_MODEL_WEIGHT \u001B[38;5;241m*\u001B[39m overall_auc) \u001B[38;5;241m+\u001B[39m ((\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m OVERALL_MODEL_WEIGHT) \u001B[38;5;241m*\u001B[39m bias_score)\n\u001B[1;32m---> 18\u001B[0m get_final_metric(\u001B[43mbias_metrics_df\u001B[49m, calculate_overall_auc(validate_df, MODEL_NAME))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'bias_metrics_df' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_overall_auc(df, model_name):\n",
    "    true_labels = df[TOXICITY_COLUMN]\n",
    "    predicted_labels = df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n",
    "    \n",
    "get_final_metric(bias_metrics_df, calculate_overall_auc(validate_df, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:28.284651300Z",
     "start_time": "2024-05-10T01:21:27.998217300Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m test \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../input/test.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m submission \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../input/sample_submission.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, index_col\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../input/test.csv'"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('../input/test.csv')\n",
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2024-05-10T01:21:28.259984400Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m submission[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mpredict(pad_text(test[TEXT_COLUMN], tokenizer))[:, \u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m      2\u001B[0m submission\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubmission.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "submission['prediction'] = model.predict(pad_text(test[TEXT_COLUMN], tokenizer))[:, 1]\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:28.631939200Z",
     "start_time": "2024-05-10T01:21:28.499695900Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:28.706894800Z",
     "start_time": "2024-05-10T01:21:28.656356300Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:29.082928600Z",
     "start_time": "2024-05-10T01:21:29.026616200Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:29.342147300Z",
     "start_time": "2024-05-10T01:21:29.279707Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:29.470845300Z",
     "start_time": "2024-05-10T01:21:29.443628700Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:29.704400500Z",
     "start_time": "2024-05-10T01:21:29.618457400Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:30.059866200Z",
     "start_time": "2024-05-10T01:21:30.006733500Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:30.166317500Z",
     "start_time": "2024-05-10T01:21:30.062808100Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:30.431124900Z",
     "start_time": "2024-05-10T01:21:30.219983700Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:30.623142600Z",
     "start_time": "2024-05-10T01:21:30.544704Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:30.738587700Z",
     "start_time": "2024-05-10T01:21:30.627970400Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:30.881373400Z",
     "start_time": "2024-05-10T01:21:30.789628200Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1375107,
     "sourceId": 12500,
     "sourceType": "competition"
    },
    {
     "datasetId": 1835,
     "sourceId": 3176,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 23026,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
