{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2024-04-24T02:00:53.631124Z",
     "iopub.status.busy": "2024-04-24T02:00:53.630786Z",
     "iopub.status.idle": "2024-04-24T02:00:55.689613Z",
     "shell.execute_reply": "2024-04-24T02:00:55.688661Z",
     "shell.execute_reply.started": "2024-04-24T02:00:53.631056Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T07:04:43.148481100Z",
     "start_time": "2024-05-10T07:04:43.107774300Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib.metadata\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and pre-process the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:00:55.693553Z",
     "iopub.status.busy": "2024-04-24T02:00:55.693292Z",
     "iopub.status.idle": "2024-04-24T02:01:23.715538Z",
     "shell.execute_reply": "2024-04-24T02:01:23.714810Z",
     "shell.execute_reply.started": "2024-04-24T02:00:55.693499Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T07:05:09.252069400Z",
     "start_time": "2024-05-10T07:04:49.146588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1804874 records\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('Data/train.csv')\n",
    "print('loaded %d records' % len(train))\n",
    "\n",
    "# Make sure all comment_text values are strings\n",
    "train['comment_text'] = train['comment_text'].astype(str) \n",
    "\n",
    "# List all identities\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "# Convert taget and identity columns to booleans\n",
    "def convert_to_bool(df, col_name):\n",
    "    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n",
    "    \n",
    "def convert_dataframe_to_bool(df):\n",
    "    bool_df = df.copy()\n",
    "    for col in ['target'] + identity_columns:\n",
    "        convert_to_bool(bool_df, col)\n",
    "    return bool_df\n",
    "\n",
    "train = convert_dataframe_to_bool(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into 80% train and 20% validate sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:01:23.717481Z",
     "iopub.status.busy": "2024-04-24T02:01:23.717163Z",
     "iopub.status.idle": "2024-04-24T02:01:25.493770Z",
     "shell.execute_reply": "2024-04-24T02:01:25.492828Z",
     "shell.execute_reply.started": "2024-04-24T02:01:23.717421Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T07:09:42.601609200Z",
     "start_time": "2024-05-10T07:09:41.394346900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1443899 train comments, 360975 validate comments\n"
     ]
    }
   ],
   "source": [
    "train_df, validate_df = model_selection.train_test_split(train, test_size=0.2)\n",
    "print('%d train comments, %d validate comments' % (len(train_df), len(validate_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a text tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:01:25.495531Z",
     "iopub.status.busy": "2024-04-24T02:01:25.495199Z",
     "iopub.status.idle": "2024-04-24T02:03:01.848534Z",
     "shell.execute_reply": "2024-04-24T02:03:01.847824Z",
     "shell.execute_reply.started": "2024-04-24T02:01:25.495454Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T07:10:51.803301Z",
     "start_time": "2024-05-10T07:09:44.845278200Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 10000\n",
    "TOXICITY_COLUMN = 'target'\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "\n",
    "# Create a text tokenizer.\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_df[TEXT_COLUMN])\n",
    "\n",
    "# All comments must be truncated or padded to be the same length.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "def pad_text(texts, tokenizer):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train a Convolutional Neural Net for classifying toxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:03:01.850248Z",
     "iopub.status.busy": "2024-04-24T02:03:01.850018Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T08:05:52.970137400Z",
     "start_time": "2024-05-10T07:10:50.577887200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embeddings\n",
      "compiling model\n",
      "training model\n",
      "Epoch 1/10\n",
      "11281/11281 - 332s - 29ms/step - acc: 0.9332 - loss: 0.1941 - val_acc: 0.9386 - val_loss: 0.1707\n",
      "Epoch 2/10\n",
      "11281/11281 - 326s - 29ms/step - acc: 0.9421 - loss: 0.1622 - val_acc: 0.9435 - val_loss: 0.1576\n",
      "Epoch 3/10\n",
      "11281/11281 - 326s - 29ms/step - acc: 0.9443 - loss: 0.1545 - val_acc: 0.9449 - val_loss: 0.1528\n",
      "Epoch 4/10\n",
      "11281/11281 - 326s - 29ms/step - acc: 0.9458 - loss: 0.1502 - val_acc: 0.9451 - val_loss: 0.1522\n",
      "Epoch 5/10\n",
      "11281/11281 - 325s - 29ms/step - acc: 0.9466 - loss: 0.1473 - val_acc: 0.9456 - val_loss: 0.1497\n",
      "Epoch 6/10\n",
      "11281/11281 - 324s - 29ms/step - acc: 0.9472 - loss: 0.1453 - val_acc: 0.9461 - val_loss: 0.1483\n",
      "Epoch 7/10\n",
      "11281/11281 - 322s - 29ms/step - acc: 0.9475 - loss: 0.1437 - val_acc: 0.9452 - val_loss: 0.1555\n",
      "Epoch 8/10\n",
      "11281/11281 - 310s - 28ms/step - acc: 0.9480 - loss: 0.1425 - val_acc: 0.9461 - val_loss: 0.1480\n",
      "Epoch 9/10\n",
      "11281/11281 - 316s - 28ms/step - acc: 0.9483 - loss: 0.1415 - val_acc: 0.9428 - val_loss: 0.1605\n",
      "Epoch 10/10\n",
      "11281/11281 - 314s - 28ms/step - acc: 0.9486 - loss: 0.1407 - val_acc: 0.9449 - val_loss: 0.1515\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_PATH = 'Embedding_file/glove.6B.100d.txt'\n",
    "EMBEDDINGS_DIMENSION = 100\n",
    "DROPOUT_RATE = 0.3\n",
    "LEARNING_RATE = 0.00005\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def train_model(train_df, validate_df, tokenizer):\n",
    "    # Prepare data\n",
    "    train_text = pad_text(train_df[TEXT_COLUMN], tokenizer)\n",
    "    train_labels = to_categorical(train_df[TOXICITY_COLUMN])\n",
    "    validate_text = pad_text(validate_df[TEXT_COLUMN], tokenizer)\n",
    "    validate_labels = to_categorical(validate_df[TOXICITY_COLUMN])\n",
    "\n",
    "    # Load embeddings\n",
    "    print('loading embeddings')\n",
    "    embeddings_index = {}\n",
    "    with open(EMBEDDINGS_PATH, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1,\n",
    "                                 EMBEDDINGS_DIMENSION))\n",
    "    num_words_in_embedding = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            num_words_in_embedding += 1\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # Create model layers.\n",
    "    def get_convolutional_neural_net_layers():\n",
    "        \"\"\"Returns (input_layer, output_layer)\"\"\"\n",
    "        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                            EMBEDDINGS_DIMENSION,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)\n",
    "\n",
    "        x = embedding_layer(sequence_input)\n",
    "        x = Conv1D(128, 2, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(5, padding='same')(x)\n",
    "        x = Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(5, padding='same')(x)\n",
    "        x = Conv1D(128, 4, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(40, padding='same')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dropout(DROPOUT_RATE)(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        preds = Dense(2, activation='softmax')(x)\n",
    "        return sequence_input, preds\n",
    "\n",
    "    # Compile model.\n",
    "    print('compiling model')\n",
    "    input_layer, output_layer = get_convolutional_neural_net_layers()\n",
    "    model = Model(input_layer, output_layer)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(learning_rate=LEARNING_RATE),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    # Train model.\n",
    "    print('training model')\n",
    "    model.fit(train_text,\n",
    "              train_labels,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=NUM_EPOCHS,\n",
    "              validation_data=(validate_text, validate_labels),\n",
    "              verbose=2)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_model(train_df, validate_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate model predictions on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T08:07:41.114863600Z",
     "start_time": "2024-05-10T08:06:28.605215300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m11281/11281\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m56s\u001B[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'my_model'\n",
    "validate_df[MODEL_NAME] = model.predict(pad_text(validate_df[TEXT_COLUMN], tokenizer))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T08:17:17.148886200Z",
     "start_time": "2024-05-10T08:17:16.968777500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              id  target                                       comment_text  \\\n720499   5003726   False  Rep. Young, many islanders are low income too....   \n1709308  6217781    True                           Jus take the damn phone.   \n579348    951090   False  It's akin to the opioid crisis that the G&M ha...   \n549291    915142   False  And the Left learned nothing, thinking that sh...   \n58349     313387   False  I've been belittled by multiple jcpd officers....   \n\n         severe_toxicity   obscene  identity_attack    insult  threat  asian  \\\n720499              0.00  0.000000              0.0  0.000000     0.0    NaN   \n1709308             0.04  0.746667              0.0  0.266667     0.0    NaN   \n579348              0.00  0.000000              0.0  0.000000     0.0    NaN   \n549291              0.00  0.000000              0.0  0.000000     0.0    NaN   \n58349               0.00  0.000000              0.0  0.000000     0.0    NaN   \n\n         atheist  ...    rating  funny  wow  sad  likes  disagree  \\\n720499       NaN  ...  approved      0    0    0      1         0   \n1709308      NaN  ...  approved      0    0    0      0         0   \n579348       NaN  ...  approved      0    0    1      2         0   \n549291       NaN  ...  approved      0    0    0      0         0   \n58349        NaN  ...  approved      0    0    0      0         0   \n\n         sexual_explicit  identity_annotator_count  toxicity_annotator_count  \\\n720499               0.0                         0                         4   \n1709308              0.0                         0                        75   \n579348               0.0                         0                         4   \n549291               0.0                         0                         4   \n58349                0.0                         0                         4   \n\n         my_model  \n720499   0.002612  \n1709308  0.931845  \n579348   0.006215  \n549291   0.122622  \n58349    0.033353  \n\n[5 rows x 46 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>comment_text</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>asian</th>\n      <th>atheist</th>\n      <th>...</th>\n      <th>rating</th>\n      <th>funny</th>\n      <th>wow</th>\n      <th>sad</th>\n      <th>likes</th>\n      <th>disagree</th>\n      <th>sexual_explicit</th>\n      <th>identity_annotator_count</th>\n      <th>toxicity_annotator_count</th>\n      <th>my_model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>720499</th>\n      <td>5003726</td>\n      <td>False</td>\n      <td>Rep. Young, many islanders are low income too....</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>approved</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0.002612</td>\n    </tr>\n    <tr>\n      <th>1709308</th>\n      <td>6217781</td>\n      <td>True</td>\n      <td>Jus take the damn phone.</td>\n      <td>0.04</td>\n      <td>0.746667</td>\n      <td>0.0</td>\n      <td>0.266667</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>approved</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>75</td>\n      <td>0.931845</td>\n    </tr>\n    <tr>\n      <th>579348</th>\n      <td>951090</td>\n      <td>False</td>\n      <td>It's akin to the opioid crisis that the G&amp;M ha...</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>approved</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0.006215</td>\n    </tr>\n    <tr>\n      <th>549291</th>\n      <td>915142</td>\n      <td>False</td>\n      <td>And the Left learned nothing, thinking that sh...</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>approved</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0.122622</td>\n    </tr>\n    <tr>\n      <th>58349</th>\n      <td>313387</td>\n      <td>False</td>\n      <td>I've been belittled by multiple jcpd officers....</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>approved</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0.033353</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 46 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define bias metrics, then evaluate our new model for bias using the validation set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T08:30:19.509577Z",
     "start_time": "2024-05-10T08:30:15.175185900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                        subgroup  subgroup_size  subgroup_auc  bpsn_auc  \\\n6                          black           2956      0.787985  0.749373   \n2      homosexual_gay_or_lesbian           2158      0.794058  0.768307   \n7                          white           5102      0.801040  0.764577   \n5                         muslim           4125      0.829505  0.800732   \n4                         jewish           1538      0.856885  0.852879   \n8  psychiatric_or_mental_illness            958      0.870528  0.839838   \n0                           male           8876      0.878722  0.858674   \n1                         female          10696      0.879187  0.867375   \n3                      christian           8051      0.902929  0.908629   \n\n   bnsp_auc  \n6  0.958682  \n2  0.955694  \n7  0.957968  \n5  0.954900  \n4  0.942829  \n8  0.955492  \n0  0.946996  \n1  0.943602  \n3  0.929281  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subgroup</th>\n      <th>subgroup_size</th>\n      <th>subgroup_auc</th>\n      <th>bpsn_auc</th>\n      <th>bnsp_auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>black</td>\n      <td>2956</td>\n      <td>0.787985</td>\n      <td>0.749373</td>\n      <td>0.958682</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>homosexual_gay_or_lesbian</td>\n      <td>2158</td>\n      <td>0.794058</td>\n      <td>0.768307</td>\n      <td>0.955694</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>white</td>\n      <td>5102</td>\n      <td>0.801040</td>\n      <td>0.764577</td>\n      <td>0.957968</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>muslim</td>\n      <td>4125</td>\n      <td>0.829505</td>\n      <td>0.800732</td>\n      <td>0.954900</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>jewish</td>\n      <td>1538</td>\n      <td>0.856885</td>\n      <td>0.852879</td>\n      <td>0.942829</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>psychiatric_or_mental_illness</td>\n      <td>958</td>\n      <td>0.870528</td>\n      <td>0.839838</td>\n      <td>0.955492</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>male</td>\n      <td>8876</td>\n      <td>0.878722</td>\n      <td>0.858674</td>\n      <td>0.946996</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>female</td>\n      <td>10696</td>\n      <td>0.879187</td>\n      <td>0.867375</td>\n      <td>0.943602</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>christian</td>\n      <td>8051</td>\n      <td>0.902929</td>\n      <td>0.908629</td>\n      <td>0.929281</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, model_name):\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n",
    "    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n",
    "    examples = pd.concat([subgroup_negative_examples, non_subgroup_positive_examples])\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df[df[subgroup] & df[label]]\n",
    "    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n",
    "    examples = pd.concat([subgroup_positive_examples, non_subgroup_negative_examples])\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {'subgroup': subgroup, 'subgroup_size': len(dataset[dataset[subgroup]]),\n",
    "                  SUBGROUP_AUC: compute_subgroup_auc(dataset, subgroup, label_col, model),\n",
    "                  BPSN_AUC: compute_bpsn_auc(dataset, subgroup, label_col, model),\n",
    "                  BNSP_AUC: compute_bnsp_auc(dataset, subgroup, label_col, model)}\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n",
    "\n",
    "bias_metrics_df = compute_bias_metrics_for_model(validate_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "bias_metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the final score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T08:30:28.592013800Z",
     "start_time": "2024-05-10T08:30:28.427418700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.8833123736521704"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_overall_auc(df, model_name):\n",
    "    true_labels = df[TOXICITY_COLUMN]\n",
    "    predicted_labels = df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n",
    "    \n",
    "get_final_metric(bias_metrics_df, calculate_overall_auc(validate_df, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T08:30:34.347941900Z",
     "start_time": "2024-05-10T08:30:33.636985700Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('Data/test.csv')\n",
    "submission = pd.read_csv('Data/sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T08:33:36.825736400Z",
     "start_time": "2024-05-10T08:32:00.225097600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m3042/3042\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m41s\u001B[0m 13ms/step\n"
     ]
    }
   ],
   "source": [
    "submission['prediction'] = model.predict(pad_text(test[TEXT_COLUMN], tokenizer))[:, 1]\n",
    "submission.to_csv('benchmark_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:28.631939200Z",
     "start_time": "2024-05-10T01:21:28.499695900Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:28.706894800Z",
     "start_time": "2024-05-10T01:21:28.656356300Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:29.082928600Z",
     "start_time": "2024-05-10T01:21:29.026616200Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:29.342147300Z",
     "start_time": "2024-05-10T01:21:29.279707Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:29.470845300Z",
     "start_time": "2024-05-10T01:21:29.443628700Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:29.704400500Z",
     "start_time": "2024-05-10T01:21:29.618457400Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:30.059866200Z",
     "start_time": "2024-05-10T01:21:30.006733500Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:30.166317500Z",
     "start_time": "2024-05-10T01:21:30.062808100Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:30.431124900Z",
     "start_time": "2024-05-10T01:21:30.219983700Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:30.623142600Z",
     "start_time": "2024-05-10T01:21:30.544704Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:30.738587700Z",
     "start_time": "2024-05-10T01:21:30.627970400Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T01:21:30.881373400Z",
     "start_time": "2024-05-10T01:21:30.789628200Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1375107,
     "sourceId": 12500,
     "sourceType": "competition"
    },
    {
     "datasetId": 1835,
     "sourceId": 3176,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 23026,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
